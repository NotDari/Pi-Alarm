{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/atulanandjha/lfwpeople\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed92ec",
   "metadata": {},
   "source": [
    "The code below downloads all the necessary data from the COCO dataset.\n",
    "It only downloads the \"person\" data, and it downloads it to the fiftyone location on the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93abe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dariushtomlinson/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fiftyOne\n",
    "import fiftyone.zoo as fiftyOneZoo\n",
    "\n",
    "#Guard just to stop accidental download\n",
    "download = False\n",
    "\n",
    "if download:\n",
    "    # Download the COCO 2017 train data\n",
    "    dataset = fiftyOneZoo.load_zoo_dataset(\n",
    "        \"coco-2017\",\n",
    "        split=\"train\",\n",
    "        max_samples=None,          \n",
    "        label_types=[\"detections\"],\n",
    "        classes=[\"person\"]\n",
    "    )\n",
    "    \n",
    "    # Download the COCO 2017 val data\n",
    "    dataset = fiftyOneZoo.load_zoo_dataset(\n",
    "        \"coco-2017\",\n",
    "        split=\"validation\",\n",
    "        max_samples=None,          \n",
    "        label_types=[\"detections\"],\n",
    "        classes=[\"person\"]\n",
    "    )\n",
    "    # Download the COCO 2017 test data\n",
    "    train_dataset = fiftyOneZoo.load_zoo_dataset(\n",
    "        \"coco-2017\",\n",
    "        split=\"test\",\n",
    "        max_samples=None,          #\n",
    "        label_types=[\"detections\"],\n",
    "        classes=[\"person\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302b40a",
   "metadata": {},
   "source": [
    "Defines the Classes needed for the data conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf8954d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Category dict where the key is the id, and the value is the category\n",
    "#Allows for conversion from id to category\n",
    "category_dict = {}\n",
    "\n",
    "#Dict which contains all the Images\n",
    "images_dict = {}\n",
    "\n",
    "class Category:\n",
    "    \"\"\"\n",
    "        Class which handles the details of each annotation class.\n",
    "        Contains id, name and supercategory.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, id):\n",
    "        \"\"\" Constructor for Category.\n",
    "\n",
    "            Parameters:\n",
    "                id (int): The id of the category\n",
    " \n",
    "        \"\"\"\n",
    "        self.id = id\n",
    "    \n",
    "    def define_details(self, name, supercategory):\n",
    "        \"\"\"\n",
    "        Setter which sets the name and supercategory.\n",
    "\n",
    "        Parameters:\n",
    "        name (string): The name of this category.\n",
    "        supercategory string): The supercategory name of this Category.\n",
    "\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.supercategory = supercategory\n",
    "\n",
    "class Annotation:\n",
    "    \"\"\"\n",
    "        Class which represents the Annotation (or label for one specific class) of an image.\n",
    "        Contains bbox details, and sizing, and how to convert that from json to a .txt.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, id):\n",
    "        \"\"\"\n",
    "            Constructor for the annotation.\n",
    "\n",
    "            Parameters:\n",
    "                id (int): The id of the annotation.\n",
    "        \"\"\"\n",
    "        self.id = id\n",
    "\n",
    "    def define_details(self,category_id,area, bbox, image_id):\n",
    "        \"\"\"\n",
    "            Setter for the details of the annotation.\n",
    "\n",
    "            Parameters:\n",
    "                category_id (int): The id of the annotation.\n",
    "                area (float): the area of the bbox\n",
    "                bbox ([float]): coordinates of the bounding box for this annotation\n",
    "                image_id (int): id of the image this annotation is for\n",
    "            Raises:\n",
    "                KeyError: If category_id is not a valid id.\n",
    "        \"\"\"\n",
    "        #Guard statement to check category is an active one\n",
    "        if category_id not in category_dict:\n",
    "            raise KeyError(f\"Category '{category_id}' not found in the dictionary\")\n",
    "        \n",
    "        self.category = category_dict[category_id]\n",
    "        self.area = area\n",
    "        self.bbox = bbox\n",
    "        self.image_id = image_id\n",
    "\n",
    "    \n",
    "    def get_yolo_line(self, image_width, image_height):\n",
    "        \"\"\"\n",
    "            Uses the image dimensions and bbox dimensions to generate the normalised bbox values as yolo expects,\n",
    "            and creates a text line as yolo expects for one of its labels.\n",
    "\n",
    "            Parameters:\n",
    "                image_width (int): The width of the image.\n",
    "                image_height (int): The height of the image.\n",
    "\n",
    "            Returns:\n",
    "                str: A single line string formatted for YOLO annotation files:\n",
    "                    \"<class_id> <center_x> <center_y> <width> <height>\"\n",
    "                    where all coordinates and sizes are normalized floats.\n",
    "\n",
    "        \"\"\"\n",
    "        #Coco bbox dimensions\n",
    "        bbox_low_x = self.bbox[0]\n",
    "        bbox_high_y = self.bbox[1]\n",
    "        bbox_width = self.bbox[2]\n",
    "        bbox_height = self.bbox[3]\n",
    "\n",
    "        #Normalised bbox dimensions\n",
    "        bbox_norm_x = (bbox_low_x + bbox_width / 2) / image_width\n",
    "        bbox_norm_y = (bbox_high_y + bbox_height / 2) / image_height\n",
    "        bbox_norm_width = bbox_width / image_width\n",
    "        bbox_norm_height = bbox_height / image_height\n",
    "\n",
    "\n",
    "        return f\"{self.category.id} {bbox_norm_x} {bbox_norm_y} {bbox_norm_width} {bbox_norm_height}\"\n",
    "\n",
    "\n",
    "class Image:\n",
    "    \"\"\" \n",
    "        A class representing one of the dataset images.\n",
    "        Contains a list of associated attributes, and a link to the image file.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, id):\n",
    "        \"\"\" \n",
    "            Constructor to define an Image with a given id.\n",
    "        \n",
    "            Parameters:\n",
    "                id (int): The id of the Image.\n",
    "        \"\"\"\n",
    "        self.image_id = id\n",
    "        self.attribute_list = []\n",
    "    \n",
    "    def define_details(self, filename, width, height):\n",
    "        \"\"\"\n",
    "            Setter for the details of the Images.\n",
    "\n",
    "            Parameters:\n",
    "                filename (string): The filename of the image.\n",
    "                width (int): the width of the image\n",
    "                height (int): the height of the image\n",
    "        \n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def add_attribute(self,attribute):\n",
    "        \"\"\" \n",
    "            Adds an attribue to the list for this image.\n",
    "\n",
    "            Parameters:\n",
    "                attribute (Attribute): Attribute to be added.\n",
    "        \"\"\"\n",
    "        self.attribute_list.append(attribute)\n",
    "    \n",
    "    def convert_to_yolo_line(self):\n",
    "        \"\"\"\n",
    "            Gets all the details required for the label.\n",
    "            \n",
    "            Returns:\n",
    "                list:\n",
    "                    A two-element list containing:\n",
    "                        [0] str: The filename of the image the annotations belong to.\n",
    "                        [1] list of str: A list of YOLO annotation lines, one per object.\n",
    "                            Each line has the format:\n",
    "                            \"<class_id> <center_x> <center_y> <width> <height>\"\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        #Get all the annotations as lines to be appended in the file\n",
    "        lines = []\n",
    "        for annotation in self.attribute_list:\n",
    "            lines.append(annotation.get_yolo_line(self.width, self.height))\n",
    "\n",
    "        return [self.filename,lines]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7826eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "json_path = \"Data/train&val/train/labels.json\"\n",
    "img_path = \"Data/train&val/train/data/\"\n",
    "\n",
    "#Load the json from the file path into data\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "#Loop through each category, creating a Category object, and append it to the category dict\n",
    "for category_json in data[\"categories\"]:\n",
    "    #Create Category\n",
    "    category = Category(category_json[\"id\"])\n",
    "    category.define_details(name=category_json[\"name\"],supercategory=category_json[\"supercategory\"])\n",
    "\n",
    "\n",
    "    #Since I am only labelling people, I am doing a check to see if its the person class\n",
    "    #Setting people category id to 0 as we are just dealing with it and yolo classes are 0 indexed.\n",
    "    if category.name == \"person\":\n",
    "        people_category_id = category.id\n",
    "        category.id = 0\n",
    "\n",
    "    #Append it\n",
    "    category_dict[category.id] = category\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#Since I am only labelling people, this is a guard to check that the people id has been noted\n",
    "try:\n",
    "    people_category_id\n",
    "except NameError:\n",
    "    raise ValueError(\"People category id is not set\")\n",
    "\n",
    "#Loop through every image, creating an Image object, and append it to the image dict\n",
    "for images_json in data[\"images\"]:\n",
    "    #Create image\n",
    "    image = Image(images_json[\"id\"])\n",
    "    image.define_details(filename=img_path+images_json[\"file_name\"],width=images_json[\"width\"],height=images_json[\"height\"])\n",
    "\n",
    "    #Append it\n",
    "    images_dict[image.image_id] = image\n",
    "\n",
    "#Loops through every annotation, creating an Annotation object, and add it to the appropriate Image object.\n",
    "for annotation_json in data[\"annotations\"]:\n",
    "    #Checking if annotation \n",
    "    if annotation_json[\"category_id\"] == people_category_id:\n",
    "        #Create annotation\n",
    "        annotation = Annotation(annotation_json[\"id\"])\n",
    "\n",
    "        #Putting 0 for the category as yolo labelling is 0 indexed and we are just dealing with people\n",
    "        annotation.define_details(category_id=0, area=annotation_json[\"area\"], bbox=annotation_json[\"bbox\"], image_id=annotation_json[\"image_id\"])\n",
    "        \n",
    "        #Add the annotation to the image\n",
    "        images_dict[annotation.image_id].add_attribute(annotation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f65414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def empty_folder(folder_path):\n",
    "    \"\"\"\n",
    "        Empties a folder and deletes all its contents including files and subfolders.\n",
    "\n",
    "        Parameters:\n",
    "            folder_path (string) : path of the folder to empty\n",
    "\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path) \n",
    "        else:\n",
    "            os.remove(file_path) \n",
    "\n",
    "set_type = \"train\"\n",
    "\n",
    "# Set the paths for both the images and labels of the dataset\n",
    "images_path = \"Data/yolo_dataset/images/\" + set_type + \"/\"\n",
    "labels_path = \"Data/yolo_dataset/labels/\" + set_type + \"/\"\n",
    "\n",
    "#Create folders if they don't exist\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "os.makedirs(labels_path, exist_ok=True)\n",
    "\n",
    "#Empty both folders\n",
    "empty_folder(images_path)\n",
    "empty_folder(labels_path)\n",
    "\n",
    "\n",
    "\n",
    "count = 1\n",
    "for image in images_dict.values():\n",
    "    [image_name, label_lines] = image.convert_to_yolo_line()\n",
    "    file_name = f\"image{count}\"\n",
    "    with open(labels_path + file_name + \".txt\", \"w\") as f:\n",
    "        for line in label_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "    shutil.copy2(image.filename, images_path + file_name + \".png\")\n",
    "\n",
    "\n",
    "\n",
    "    count = count + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f029086",
   "metadata": {},
   "source": [
    "This code creates the yolo nano model if it doesn't exist currently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afeecfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.235 üöÄ Python-3.11.5 torch-2.9.1 MPS (Apple M2 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset.yaml, degrees=0.0, deterministic=True, device=mps, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/dariushtomlinson/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 348.8¬±48.5 MB/s, size: 168.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/dariushtomlinson/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/Dataset/labels/train... 64115 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 64115/64115 4.8Kit/s 13.3s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/dariushtomlinson/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/Dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.2¬±0.0 ms, read: 413.3¬±168.2 MB/s, size: 104.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/dariushtomlinson/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/Dataset/labels/val... 2693 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2693/2693 5.2Kit/s 0.5s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/dariushtomlinson/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/Dataset/labels/val.cache\n",
      "Plotting labels to /Users/dariushtomlinson/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/dariushtomlinson/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/runs/detect/train3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50      4.28G      1.154      2.911       1.18        162        640: 1% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 51/4008 1.1it/s 1:10<58:15381\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#Get nano model\u001b[39;00m\n\u001b[32m      4\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33myolov8n.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/venv/lib/python3.11/site-packages/ultralytics/engine/model.py:773\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    771\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/venv/lib/python3.11/site-packages/ultralytics/engine/trainer.py:243\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    240\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/venv/lib/python3.11/site-packages/ultralytics/engine/trainer.py:434\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28mself\u001b[39m.tloss = \u001b[38;5;28mself\u001b[39m.loss_items \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.tloss * i + \u001b[38;5;28mself\u001b[39m.loss_items) / (i + \u001b[32m1\u001b[39m)\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Documents - Dariush‚Äôs MacBook Pro/Exeter/Computer Science/Year 3/Dissertation/Project/Pi-Alarm/Models/venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "#Get nano model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "model.train(\n",
    "    data=\"dataset.yaml\",\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device= \"mps\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
